{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xjM8PRnJcVWj",
   "metadata": {
    "id": "xjM8PRnJcVWj"
   },
   "source": [
    "We will use zero-shot baseline: [OVSeg](https://github.com/facebookresearch/ov-seg), which utilizes [SAM](https://github.com/facebookresearch/segment-anything) as a proposal generator instead of MaskFormer in the original setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cf8N2eTJcXti",
   "metadata": {
    "id": "Cf8N2eTJcXti"
   },
   "source": [
    "Let's download models and install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cedaaff",
   "metadata": {
    "id": "8cedaaff"
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/spaces/facebook/ov-seg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gvhY_K8QM6jh",
   "metadata": {
    "id": "gvhY_K8QM6jh"
   },
   "outputs": [],
   "source": [
    "cd ov-seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6jwBnO3yPE8x",
   "metadata": {
    "id": "6jwBnO3yPE8x"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip -q install -r requirements.txt\n",
    "!pip -q install typing-extensions --upgrade\n",
    "!pip -q install scipy --upgrade\n",
    "!pip -q install gradio timm ftfy wandb open_clip_torch==1.3.0 git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip -q install git+https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed0e94",
   "metadata": {
    "cellId": "zbcp10n5bsqm1outm6lu18",
    "id": "2bed0e94"
   },
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from detectron2.structures import BitMasks\n",
    "from open_vocab_seg.modeling.clip_adapter.adapter import PIXEL_MEAN, PIXEL_STD\n",
    "from open_vocab_seg.modeling.clip_adapter.utils import crop_with_mask\n",
    "import cv2\n",
    "import torch\n",
    "import open_clip\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry \n",
    "from torch.nn import functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b845149",
   "metadata": {
    "cellId": "oce8tl37z5iufsqzblg9h8",
    "id": "9b845149"
   },
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "    # Taken from https://stackoverflow.com/a/42874377\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x, y) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "    assert bb1['x1'] < bb1['x2']\n",
    "    assert bb1['y1'] < bb1['y2']\n",
    "    assert bb2['x1'] < bb2['x2']\n",
    "    assert bb2['y1'] < bb2['y2']\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_image_array(url):\n",
    "    response = requests.get(url)\n",
    "    img_bytes = BytesIO(response.content)\n",
    "    img_cv2 = cv2.imdecode(np.frombuffer(img_bytes.read(), np.uint8), -1)\n",
    "    img_np = np.asarray(img_cv2)\n",
    "    return img_np\n",
    "\n",
    "\n",
    "def get_models(sam_path, ovsegclip_path, model_type):\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_path).cuda()\n",
    "    predictor = SamAutomaticMaskGenerator(sam, points_per_batch=16)\n",
    "    clip_model, _, _ = open_clip.create_model_and_transforms('ViT-L-14', pretrained=ovsegclip_path)\n",
    "    clip_model.cuda()\n",
    "    return predictor, clip_model\n",
    "\n",
    "\n",
    "def get_mask(url, question, granularity, predictor, clip_model):\n",
    "    class_names = [question]\n",
    "    img = get_image_array(url)\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        masks = predictor.generate(image)\n",
    "\n",
    "    pred_masks = [masks[i]['segmentation'][None,:,:] for i in range(len(masks))]\n",
    "    pred_masks = np.row_stack(pred_masks)\n",
    "    pred_masks = BitMasks(pred_masks)\n",
    "    bboxes = pred_masks.get_bounding_boxes()\n",
    "\n",
    "    mask_fill = [255.0 * c for c in PIXEL_MEAN]\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    regions = []\n",
    "    for bbox, mask in zip(bboxes, pred_masks):\n",
    "        region, _ = crop_with_mask(\n",
    "            image,\n",
    "            mask,\n",
    "            bbox,\n",
    "            fill=mask_fill,\n",
    "        )\n",
    "        regions.append(region.unsqueeze(0))\n",
    "    regions = [F.interpolate(r.to(torch.float), size=(224, 224), mode=\"bicubic\") for r in regions]\n",
    "\n",
    "    pixel_mean = torch.tensor(PIXEL_MEAN).reshape(1, -1, 1, 1)\n",
    "    pixel_std = torch.tensor(PIXEL_STD).reshape(1, -1, 1, 1)\n",
    "    imgs = [(r/255.0 - pixel_mean) / pixel_std for r in regions]\n",
    "    imgs = torch.cat(imgs)\n",
    "    if len(class_names) == 1:\n",
    "        class_names.append('others')\n",
    "    txts = [f'a photo of {cls_name}' for cls_name in class_names]\n",
    "    text = open_clip.tokenize(txts)\n",
    "\n",
    "    img_batches = torch.split(imgs, 32, dim=0)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = clip_model.encode_text(text.cuda())\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = []\n",
    "        for img_batch in img_batches:\n",
    "            image_feat = clip_model.encode_image(img_batch.cuda().half())\n",
    "            image_feat /= image_feat.norm(dim=-1, keepdim=True)\n",
    "            image_features.append(image_feat.detach())\n",
    "        image_features = torch.cat(image_features, dim=0)\n",
    "        class_preds = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    select_cls = torch.zeros_like(class_preds)\n",
    "\n",
    "    max_scores, select_mask = torch.max(class_preds, dim=0)\n",
    "    if len(class_names) == 2 and class_names[-1] == 'others':\n",
    "        select_mask = select_mask[:-1]\n",
    "    if granularity < 1:\n",
    "        thr_scores = max_scores * granularity\n",
    "        select_mask = []\n",
    "        if len(class_names) == 2 and class_names[-1] == 'others':\n",
    "            thr_scores = thr_scores[:-1]\n",
    "        for i, thr in enumerate(thr_scores):\n",
    "            cls_pred = class_preds[:,i]\n",
    "            locs = torch.where(cls_pred > thr)\n",
    "            select_mask.extend(locs[0].tolist())\n",
    "    for idx in select_mask:\n",
    "        select_cls[idx] = class_preds[idx]\n",
    "    semseg = torch.einsum(\"qc,qhw->chw\", select_cls.float(), pred_masks.tensor.float().cuda())\n",
    "\n",
    "    r = semseg\n",
    "    blank_area = (r[0] == 0)\n",
    "    pred_mask = r.argmax(dim=0).to('cpu')\n",
    "    pred_mask[~blank_area] = 1\n",
    "    pred_mask = np.array(pred_mask, dtype=int)\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def get_bounding_box(mask: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Get the bounding box of a segmentation mask in the form of a NumPy bool array.\n",
    "    \n",
    "    Args:\n",
    "        mask (NumPy array): The segmentation mask as a NumPy bool array.\n",
    "        \n",
    "    Returns:\n",
    "        A NumPy array of the bounding box in the format [left, top, right, bottom].\n",
    "    \"\"\"\n",
    "\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    left, right = np.where(cols)[0][[0, -1]]\n",
    "    top, bottom = np.where(rows)[0][[0, -1]]\n",
    "\n",
    "    return np.array([left, top, right, bottom])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_w6Gtoo-be3z",
   "metadata": {
    "id": "_w6Gtoo-be3z"
   },
   "source": [
    "The models require CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bffc719",
   "metadata": {
    "cellId": "ytmp5yf5tj6nicncfgfi",
    "execution_id": "87f85f23-97b5-4785-ba44-61540f9a0b30",
    "id": "4bffc719"
   },
   "outputs": [],
   "source": [
    "predictor, clip_model = get_models('./sam_vit_h_4b8939.pth', './ovseg_clip_l_9a1909.pth', 'vit_h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EuZH0j-4Yz0W",
   "metadata": {
    "id": "EuZH0j-4Yz0W"
   },
   "source": [
    "Let's take a look at the data. We have eight columns: image URL, its width and height in pixels, positions of the top-left and bottom-right corners of a bounding box and the question to be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ea10",
   "metadata": {
    "cellId": "ihl6wnx2vw956uhxac4p8h",
    "id": "4ad3ea10"
   },
   "outputs": [],
   "source": [
    "test_private = pd.read_csv('https://raw.githubusercontent.com/Toloka/WSDMCup2023/main/test_private.csv')\n",
    "test_private.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bX8GViB4Y42o",
   "metadata": {
    "id": "bX8GViB4Y42o"
   },
   "source": [
    "`image` and `question` are input columns. A human asked this question about some object located in this image. So, our goal is to find this object. In other words, `left`, `top`, `right`, and `bottom` are *target variables* we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llUbOR1CZRes",
   "metadata": {
    "id": "llUbOR1CZRes"
   },
   "source": [
    "Let's run a prediction for the test private set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obyg0zIuUP1Q",
   "metadata": {
    "id": "obyg0zIuUP1Q"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "n_imgs = 0\n",
    "total_iou = 0.0\n",
    "progress = tqdm(test_private.iterrows(), total=len(test_private))\n",
    "for _, row in progress:\n",
    "    img_url = row['image']\n",
    "    question = row['question']\n",
    "    gt_box = np.array([row['left'], row['top'], row['right'], row['bottom']])\n",
    "    try:\n",
    "        pred_mask = get_mask(img_url, question, 1.0, predictor, clip_model)\n",
    "    except Exception:\n",
    "        continue\n",
    "        \n",
    "    gt_bb = {'x1': row['left'], 'y1': row['top'], 'x2': row['right'], 'y2': row['bottom']}\n",
    "    bb_predicted = get_bounding_box(pred_mask)\n",
    "    bb_predicted = {'x1': bb_predicted[0], 'y1': bb_predicted[1], 'x2': bb_predicted[2], 'y2': bb_predicted[3]}\n",
    "    total_iou += get_iou(gt_bb, bb_predicted)\n",
    "    n_imgs += 1\n",
    "    progress.set_description(f'IoU: {round(total_iou / n_imgs * 100, 2)}')\n",
    "    \n",
    "    left = bb_predicted['x1']\n",
    "    top = bb_predicted['y1']\n",
    "    right = bb_predicted['x2']\n",
    "    bottom = bb_predicted['y2']\n",
    "    predictions.append([img_url, left, top, right, bottom])\n",
    "predictions = pd.DataFrame(predictions, columns=['image', 'left', 'top', 'right', 'bottom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tJliK2WrZU2X",
   "metadata": {
    "id": "tJliK2WrZU2X"
   },
   "source": [
    "Let's look at the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SSG6_eGsW6zT",
   "metadata": {
    "id": "SSG6_eGsW6zT"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ylWxS3eQZV9h",
   "metadata": {
    "id": "ylWxS3eQZV9h"
   },
   "source": [
    "...and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZJbuXo6fW8z5",
   "metadata": {
    "id": "ZJbuXo6fW8z5"
   },
   "outputs": [],
   "source": [
    "predictions.to_csv('ovseg_sam_result.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notebookId": "ccb821c1-5762-49f4-b3ec-6a1717bb61fa",
  "notebookPath": "ov-seg/ov_seg.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
